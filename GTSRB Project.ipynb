{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7a0679-d460-48b9-93c3-23f590e8a74c",
   "metadata": {},
   "source": [
    "## GTSRB Traffic Sign Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2a1c6-8016-42c5-84b1-861ab66b0562",
   "metadata": {},
   "source": [
    "### 1. Loading and Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b47873-227b-4ad1-be71-b3c7a9256bf4",
   "metadata": {},
   "source": [
    "#### 1.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ef9f11-b066-4625-a996-96857a2cf40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary libraries\n",
    "import pandas as pd #to load .csv files\n",
    "from PIL import Image # to read .png file and resize them\n",
    "import os # to locate files \n",
    "import numpy as np # to store PIL images as arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29819222-762c-44db-9d8f-6521a6fd912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"Train.csv\") ##labels of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509d0a52-d160-46f4-b62f-4dbaaa7501a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Roi.X1</th>\n",
       "      <th>Roi.Y1</th>\n",
       "      <th>Roi.X2</th>\n",
       "      <th>Roi.Y2</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00004.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39204</th>\n",
       "      <td>52</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00025.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39205</th>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00026.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39206</th>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>57</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00027.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39207</th>\n",
       "      <td>63</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>58</td>\n",
       "      <td>63</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00028.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39208</th>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00029.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39209 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Width  Height  Roi.X1  Roi.Y1  Roi.X2  Roi.Y2  ClassId  \\\n",
       "0         27      26       5       5      22      20       20   \n",
       "1         28      27       5       6      23      22       20   \n",
       "2         29      26       6       5      24      21       20   \n",
       "3         28      27       5       6      23      22       20   \n",
       "4         28      26       5       5      23      21       20   \n",
       "...      ...     ...     ...     ...     ...     ...      ...   \n",
       "39204     52      56       5       6      47      51       42   \n",
       "39205     56      58       5       5      51      53       42   \n",
       "39206     58      62       5       6      53      57       42   \n",
       "39207     63      69       5       7      58      63       42   \n",
       "39208     68      69       7       6      62      63       42   \n",
       "\n",
       "                                 Path  \n",
       "0      Train/20/00020_00000_00000.png  \n",
       "1      Train/20/00020_00000_00001.png  \n",
       "2      Train/20/00020_00000_00002.png  \n",
       "3      Train/20/00020_00000_00003.png  \n",
       "4      Train/20/00020_00000_00004.png  \n",
       "...                               ...  \n",
       "39204  Train/42/00042_00007_00025.png  \n",
       "39205  Train/42/00042_00007_00026.png  \n",
       "39206  Train/42/00042_00007_00027.png  \n",
       "39207  Train/42/00042_00007_00028.png  \n",
       "39208  Train/42/00042_00007_00029.png  \n",
       "\n",
       "[39209 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##This csv file contains class ID of images to classify them and their paths to connect them to their class ID in this csv file.\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3143bc1-d046-4805-a856-3dc18ca5b6d4",
   "metadata": {},
   "source": [
    "#### 1.2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629d33b7-ac33-4b73-9c60-d3781a29cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now loading images by Path in train_df\n",
    "base_directory= \"Downloads/GTSRB dataset/\"\n",
    "images=[]\n",
    "classID=[]\n",
    "for i in train_df[\"Path\"]:\n",
    "    image_path=os.path.join(base_directory,i) #locates each image \n",
    "    img= Image.open(image_path) # loads each image in memory for operations like resize\n",
    "    img=img.resize((32,32)) # resize each image to 32*32 pixels \n",
    "    img_array=np.array(img) # convert img to array so DL models can work with it\n",
    "    images.append(img_array) #store in a list\n",
    "    # get corresponding class ID of the images\n",
    "    ID=train_df.loc[train_df[\"Path\"]==i,\"ClassId\"].values[0]\n",
    "    #store in a list\n",
    "    classID.append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792137fc-f975-45e8-a45d-15ad498bf15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 3)\n",
      "After Normalization:  (39209, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#checking if images are of write size (32,32,3) 32* 32 pixels for each and 3 is for color channels (RGB). 39209 is total number of images. \n",
    "print(np.array(images).shape)\n",
    "# Normalize images. It means scaling images; it is done so that models train faster and provide more consistent and correct outputs. \n",
    "images=list(np.array(images)/255)\n",
    "# check again\n",
    "print(\"After Normalization: \",np.array(images).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83baea2a-aa10-48a3-9f4c-2c31347d805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "#no data was lost and images have been normalized which can be checked by\n",
    "print(images[0].max())\n",
    "#maximum should be less than 1 as normalized values lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74177354-9b6a-4640-9843-1846072c7b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>[[[0.1803921568627451, 0.2, 0.2196078431372549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>[[[0.1843137254901961, 0.19607843137254902, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>[[[0.2, 0.19607843137254902, 0.223529411764705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>[[[0.19215686274509805, 0.19215686274509805, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>[[[0.19607843137254902, 0.19607843137254902, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39204</th>\n",
       "      <td>42</td>\n",
       "      <td>[[[0.047058823529411764, 0.043137254901960784,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39205</th>\n",
       "      <td>42</td>\n",
       "      <td>[[[0.043137254901960784, 0.03529411764705882, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39206</th>\n",
       "      <td>42</td>\n",
       "      <td>[[[0.043137254901960784, 0.043137254901960784,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39207</th>\n",
       "      <td>42</td>\n",
       "      <td>[[[0.050980392156862744, 0.050980392156862744,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39208</th>\n",
       "      <td>42</td>\n",
       "      <td>[[[0.043137254901960784, 0.0392156862745098, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39209 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Class ID                                             Images\n",
       "0            20  [[[0.1803921568627451, 0.2, 0.2196078431372549...\n",
       "1            20  [[[0.1843137254901961, 0.19607843137254902, 0....\n",
       "2            20  [[[0.2, 0.19607843137254902, 0.223529411764705...\n",
       "3            20  [[[0.19215686274509805, 0.19215686274509805, 0...\n",
       "4            20  [[[0.19607843137254902, 0.19607843137254902, 0...\n",
       "...         ...                                                ...\n",
       "39204        42  [[[0.047058823529411764, 0.043137254901960784,...\n",
       "39205        42  [[[0.043137254901960784, 0.03529411764705882, ...\n",
       "39206        42  [[[0.043137254901960784, 0.043137254901960784,...\n",
       "39207        42  [[[0.050980392156862744, 0.050980392156862744,...\n",
       "39208        42  [[[0.043137254901960784, 0.0392156862745098, 0...\n",
       "\n",
       "[39209 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing images and their corresponding class ID together\n",
    "df=pd.DataFrame({\n",
    "    \"Class ID\": classID,\n",
    "    \"Images\": images\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf169f01-544d-40fb-8be4-e8845b9db867",
   "metadata": {},
   "source": [
    "##### Train test split and converting to tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5866fa-f2e7-4268-870a-d8a8361043e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries \n",
    "from sklearn.model_selection import train_test_split #to split data for training and validating\n",
    "import torch #to convert data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a861398b-0d97-4a6a-81ef-d3e355de2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split \n",
    "x_train,x_valid,y_train,y_valid=train_test_split(images,classID,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35717c86-f656-4208-ad03-81c3cbb9ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert training and validating data to tensors\n",
    "#converting to numpy array for faster tensor conversion \n",
    "x_train=np.array(x_train)\n",
    "x_valid=np.array(x_valid)\n",
    "y_train=np.array(y_train)\n",
    "y_valid=np.array(y_valid)\n",
    "#converting to tensor \n",
    "x_train_tensor=torch.tensor(x_train,dtype=torch.float32)\n",
    "x_valid_tensor=torch.tensor(x_valid,dtype=torch.float32)\n",
    "y_train_tensor=torch.tensor(y_train,dtype=torch.int64)\n",
    "y_valid_tensor=torch.tensor(y_valid,dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e56b15-0de5-43cb-b090-c8b5050c1bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before(no. of images,height,width,channels):  torch.Size([31367, 32, 32, 3])\n",
      "Shape after change(no. of images,channels,height,width):  torch.Size([31367, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# pytorch expects tensor shape as (no. of images,channels,height,width)\n",
    "print(\"Shape before(no. of images,height,width,channels): \",x_train_tensor.shape)\n",
    "#changing positions:\n",
    "x_train_tensor=x_train_tensor.permute(0,3,1,2)\n",
    "x_valid_tensor=x_valid_tensor.permute(0,3,1,2)\n",
    "print(\"Shape after change(no. of images,channels,height,width): \",x_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2099be1-5648-4656-8d78-3d3e55cba950",
   "metadata": {},
   "source": [
    "##### Creating Train dataset, valid dataset and batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26c6303e-4d6a-401e-b6eb-a9710e84843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary libraries \n",
    "from torch.utils.data import DataLoader # for loading data in batches and shuffling\n",
    "from torch.utils.data import TensorDataset # for creating train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0fa269-3c47-4c52-8413-93335dc75887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor dataset pairs images and classID together\n",
    "#creating train dataset \n",
    "train_dataset=TensorDataset(x_train_tensor,y_train_tensor)\n",
    "#creating valid dataset\n",
    "valid_dataset=TensorDataset(x_valid_tensor,y_valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3baf368f-ccdc-4836-a373-9bcc9121fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating batches in dataset for faster loading of data for models to train(by parallel processing) and shuffling the training data for each epoch. \n",
    "train_loader=DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "valid_loader=DataLoader(valid_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81d66d63-f873-4fbe-8825-76948c695c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#check if shape format is correct\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)  # Should print (batch_size, 3, 32, 32)\n",
    "    print(labels.shape)  # Should print (batch_size,)\n",
    "    break  # to inspect a single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921582a7-bb80-435f-9b02-641bd12400d0",
   "metadata": {},
   "source": [
    "### 2. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0364e0-7eb3-4b8c-bd7b-7d930761b70e",
   "metadata": {},
   "source": [
    "#### 2.1 Model definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acadcdc8-1ab6-4f80-aa3f-0f606034259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import torch.nn as nn # to define CNN model\n",
    "from torch.optim import Adam # An algorithm to optimize learning of model from loss function\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90edb791-ec4f-481b-856c-09748284b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model\n",
    "def get_cnn_model():\n",
    "    return nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1), #first layer\n",
    "    #applies 32 filters to images and thus outputs 32 filter maps of every image\n",
    "    nn.BatchNorm2d(32), #normalizes the data to improve training speed\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1), #hidden layer\n",
    "    #outputs 64 higher level filter maps of every image from the previous 32 filter maps\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=64,out_channels=96,kernel_size=3,padding=1),\n",
    "    #hidden layer. outputs 96 higher level filter maps of every image from previous 64 filter maps\n",
    "    nn.BatchNorm2d(96),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2), #divides the height and width of each filter maps of every image i.e. 32*32 to 16*16.\n",
    "    nn.Dropout(0.3), #disables (drops) 30% of the neuron randomly to avoid overfitting.\n",
    "    nn.Flatten(), #3d to 1D i.e. 64 features * 16 height *16 width for dense layers(nn.linear)\n",
    "    nn.Linear(in_features=96*16*16,out_features=128), #final connected layer\n",
    "    #combines all learned features across all layers\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128,out_features=43) #ouput layer; predicts images to 43 output classes of road sign in data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4e3ad-4214-4c7a-bafb-dc7ee5774e09",
   "metadata": {},
   "source": [
    "#### 2.2 Model Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e600d15-b592-44eb-9cb3-d04c006686ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train model\n",
    "def train_model(train_loader,valid_loader,CNN_model,epoch):\n",
    "    #train the model\n",
    "    #define loss function\n",
    "    loss_func=nn.CrossEntropyLoss()\n",
    "    #optimizer\n",
    "    optimizer=Adam(CNN_model.parameters()) \n",
    "    total_training_time=0 # to calculate training time\n",
    "    start_time=time.time()\n",
    "    for i in range(epoch):\n",
    "        #training\n",
    "        CNN_model.train()\n",
    "        true_labels=[] # to keep track of all true labels for analysis\n",
    "        predicted_labels=[] # to keep track of all predicted labels for analysis\n",
    "        for images,classID in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs=CNN_model(images)  #predicts\n",
    "            loss=loss_func(outputs,classID) #calculates loss\n",
    "            loss.backward() #learns from the made predictions\n",
    "            optimizer.step() #optimized learning by Adam (updates weights)\n",
    "        \n",
    "        #Validating\n",
    "        CNN_model.eval()\n",
    "        correct=0\n",
    "        total=0\n",
    "        with torch.no_grad():\n",
    "            for images,classid in valid_loader:\n",
    "                outputs=CNN_model(images)\n",
    "                _,predicted=torch.max(outputs,1)\n",
    "                predicted_labels.extend(predicted.tolist())\n",
    "                true_labels.extend(classid.tolist())\n",
    "                correct+=(predicted==classid).sum().item()\n",
    "                total+=classid.size(0)\n",
    "    accuracy=(correct/total)*100\n",
    "    end_time=time.time()\n",
    "    total_training_time+=(end_time-start_time)\n",
    "\n",
    "    return accuracy,total_training_time,predicted_labels,true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e47c0e-b9a2-466e-bb48-c3aa48d9c746",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "181bf8d7-2750-4c9d-95f5-6cffed3e835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "CNN=get_cnn_model()\n",
    "accuracy_training,total_training_time,predicted_labels,true_labels=train_model(train_loader,valid_loader,CNN,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673a043-7a2d-4f43-b807-87fa4085bcb2",
   "metadata": {},
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bfe12-77ce-43e9-a2c4-14ea3886dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#classification report can calculate and return a structured table of precision,recall and F1-score per class\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16c0d3d3-51c9-460d-a71e-3417e2366f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.983945</td>\n",
       "      <td>0.979452</td>\n",
       "      <td>0.981693</td>\n",
       "      <td>438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.990930</td>\n",
       "      <td>0.977629</td>\n",
       "      <td>0.984234</td>\n",
       "      <td>447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872964</td>\n",
       "      <td>0.988930</td>\n",
       "      <td>0.927336</td>\n",
       "      <td>271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952719</td>\n",
       "      <td>0.975787</td>\n",
       "      <td>423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.964187</td>\n",
       "      <td>366.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.984962</td>\n",
       "      <td>0.919298</td>\n",
       "      <td>0.950998</td>\n",
       "      <td>285.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.928328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962832</td>\n",
       "      <td>272.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.992537</td>\n",
       "      <td>0.983364</td>\n",
       "      <td>268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.970732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985149</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992620</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.992754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995516</td>\n",
       "      <td>0.997753</td>\n",
       "      <td>446.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.993506</td>\n",
       "      <td>155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.995745</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>219.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.976959</td>\n",
       "      <td>0.995305</td>\n",
       "      <td>0.986047</td>\n",
       "      <td>213.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.980892</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.993464</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.996865</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969925</td>\n",
       "      <td>0.984733</td>\n",
       "      <td>133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.983051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.952830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.993671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.986784</td>\n",
       "      <td>113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995392</td>\n",
       "      <td>0.997691</td>\n",
       "      <td>217.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.998870</td>\n",
       "      <td>443.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.989038</td>\n",
       "      <td>0.984069</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>7842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.985008</td>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984276</td>\n",
       "      <td>7842.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              1.000000  0.888889  0.941176    36.000000\n",
       "1              0.983945  0.979452  0.981693   438.000000\n",
       "2              0.990930  0.977629  0.984234   447.000000\n",
       "3              0.872964  0.988930  0.927336   271.000000\n",
       "4              1.000000  0.952719  0.975787   423.000000\n",
       "5              0.972222  0.956284  0.964187   366.000000\n",
       "6              1.000000  1.000000  1.000000    93.000000\n",
       "7              0.984962  0.919298  0.950998   285.000000\n",
       "8              0.928328  1.000000  0.962832   272.000000\n",
       "9              0.974359  0.992537  0.983364   268.000000\n",
       "10             0.970732  1.000000  0.985149   398.000000\n",
       "11             1.000000  0.992620  0.996296   271.000000\n",
       "12             0.992754  1.000000  0.996364   411.000000\n",
       "13             1.000000  0.995516  0.997753   446.000000\n",
       "14             1.000000  0.987097  0.993506   155.000000\n",
       "15             1.000000  0.991525  0.995745   118.000000\n",
       "16             1.000000  0.938776  0.968421    98.000000\n",
       "17             0.995434  0.995434  0.995434   219.000000\n",
       "18             0.976959  0.995305  0.986047   213.000000\n",
       "19             0.976190  1.000000  0.987952    41.000000\n",
       "20             1.000000  0.962500  0.980892    80.000000\n",
       "21             1.000000  1.000000  1.000000    69.000000\n",
       "22             1.000000  0.987013  0.993464    77.000000\n",
       "23             1.000000  0.969697  0.984615    99.000000\n",
       "24             1.000000  0.970149  0.984848    67.000000\n",
       "25             0.996865  0.996865  0.996865   319.000000\n",
       "26             1.000000  0.969925  0.984733   133.000000\n",
       "27             1.000000  1.000000  1.000000    47.000000\n",
       "28             1.000000  1.000000  1.000000   125.000000\n",
       "29             0.983051  1.000000  0.991453    58.000000\n",
       "30             0.952830  1.000000  0.975845   101.000000\n",
       "31             0.993671  1.000000  0.996825   157.000000\n",
       "32             1.000000  1.000000  1.000000    39.000000\n",
       "33             0.982456  0.991150  0.986784   113.000000\n",
       "34             1.000000  0.958333  0.978723    72.000000\n",
       "35             1.000000  0.995392  0.997691   217.000000\n",
       "36             1.000000  1.000000  1.000000    73.000000\n",
       "37             1.000000  0.979592  0.989691    49.000000\n",
       "38             1.000000  0.997743  0.998870   443.000000\n",
       "39             1.000000  0.984615  0.992248    65.000000\n",
       "40             1.000000  1.000000  1.000000    71.000000\n",
       "41             1.000000  1.000000  1.000000    54.000000\n",
       "42             1.000000  1.000000  1.000000    45.000000\n",
       "accuracy       0.984188  0.984188  0.984188     0.984188\n",
       "macro avg      0.989038  0.984069  0.986228  7842.000000\n",
       "weighted avg   0.985008  0.984188  0.984276  7842.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report=classification_report(true_labels,predicted_labels,output_dict=True)\n",
    "report=pd.DataFrame(report).T\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab53784-f758-482f-97e9-2628b7016c3d",
   "metadata": {},
   "source": [
    "#### Analysis of classification report\n",
    "                                         The classification report shows that the model is robust and has a good accuracy. However, training classification report can not be used to evaluate the model alone. The training classification report shows its true value when compared with the test classification report where it helps identify weaker classes.                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e62316b3-289f-41df-94d3-4878f5d7e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training time of Model:  17.400458467006683 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1044.027508020401"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total Training time of Model: \",total_training_time/60,\"minutes\")\n",
    "total_training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e10bc-bedd-4f3d-9f39-dfe8f505768d",
   "metadata": {},
   "source": [
    "### 3. Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eed066-7813-4729-a7a5-8568401063be",
   "metadata": {},
   "source": [
    "#### 3.1 Preprocessing testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6ebda5-988c-4724-9709-e84eb603d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv(\"Test.csv\")\n",
    "test_direc=\"Downloads/GTSRB dataset/\"\n",
    "test_images=[]\n",
    "labels=[]\n",
    "for i in test_df[\"Path\"]:\n",
    "    image_Path=os.path.join(test_direc,i)\n",
    "    image=Image.open(image_Path)\n",
    "    image=image.resize((32,32)) #resize\n",
    "    image=np.array(image)/255 #normalization\n",
    "    test_images.append(image)\n",
    "    label=test_df.loc[test_df[\"Path\"]==i,\"ClassId\"].values[0]\n",
    "    labels.append(label)\n",
    "#transform to tensor\n",
    "labels=np.array(labels)\n",
    "test_images=np.array(test_images)\n",
    "testimage_tensor=torch.tensor(test_images,dtype=torch.float32)\n",
    "testlabel_tensor=torch.tensor(labels,dtype=torch.long)\n",
    "#permute to correct size\n",
    "testimage_tensor=testimage_tensor.permute(0,3,1,2)\n",
    "#Tensor dataset\n",
    "test_dataset=TensorDataset(testimage_tensor,testlabel_tensor)\n",
    "#dataloader\n",
    "test_loader=DataLoader(test_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcc996-0f38-4817-b0da-2e60e90a8b5a",
   "metadata": {},
   "source": [
    "#### 3.2 Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "369e8c92-3d06-4860-911b-bd2c7f6c280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(CNN_model,test_loader):\n",
    "    correct=0\n",
    "    total=0\n",
    "    true_label=[]\n",
    "    predicted_label=[]\n",
    "    CNN_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image,label in test_loader:\n",
    "            outputs=CNN_model(image)\n",
    "            _,predicts=torch.max(outputs,1)\n",
    "            predicted_label.extend(predicts.tolist())\n",
    "            true_label.extend(label.tolist())\n",
    "            correct+=(predicts==label).sum().item()\n",
    "            total+=label.size(0)\n",
    "    accuracy=(correct/total)*100\n",
    "    return accuracy,predicted_label,true_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9c71f-8891-44e4-90f6-3c33bbb698d7",
   "metadata": {},
   "source": [
    "#### 3.3 Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54808698-fa45-4d41-9686-7b0bc9ae6c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy,predicted_label,true_label=model_test(CNN,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1019b-b9d2-491b-aab8-93b4971d847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",accuracy)\n",
    "test_report=classification_report(true_label,predicted_label,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a10462d-eae4-4292-b7c4-654229fdac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.923280</td>\n",
       "      <td>0.969444</td>\n",
       "      <td>0.945799</td>\n",
       "      <td>720.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991465</td>\n",
       "      <td>0.929333</td>\n",
       "      <td>0.959394</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.989547</td>\n",
       "      <td>0.860606</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.907760</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.944402</td>\n",
       "      <td>630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.992063</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.905797</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.911162</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.813996</td>\n",
       "      <td>0.982222</td>\n",
       "      <td>0.890232</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.956697</td>\n",
       "      <td>480.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.926657</td>\n",
       "      <td>0.995455</td>\n",
       "      <td>0.959825</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.953052</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.959811</td>\n",
       "      <td>420.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>720.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>0.998145</td>\n",
       "      <td>270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.985782</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.988124</td>\n",
       "      <td>210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980556</td>\n",
       "      <td>0.990182</td>\n",
       "      <td>360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.947977</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.923848</td>\n",
       "      <td>0.960417</td>\n",
       "      <td>0.941777</td>\n",
       "      <td>480.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.750789</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.981618</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.985240</td>\n",
       "      <td>270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.983607</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.921053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.969957</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.984058</td>\n",
       "      <td>0.986919</td>\n",
       "      <td>690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.950055</td>\n",
       "      <td>0.924404</td>\n",
       "      <td>0.931984</td>\n",
       "      <td>12630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.949823</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945246</td>\n",
       "      <td>12630.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              1.000000  0.383333  0.554217     60.000000\n",
       "1              0.923280  0.969444  0.945799    720.000000\n",
       "2              0.991465  0.929333  0.959394    750.000000\n",
       "3              0.814607  0.966667  0.884146    450.000000\n",
       "4              0.989547  0.860606  0.920583    660.000000\n",
       "5              0.907760  0.984127  0.944402    630.000000\n",
       "6              0.992063  0.833333  0.905797    150.000000\n",
       "7              0.934579  0.888889  0.911162    450.000000\n",
       "8              0.813996  0.982222  0.890232    450.000000\n",
       "9              0.925926  0.989583  0.956697    480.000000\n",
       "10             0.926657  0.995455  0.959825    660.000000\n",
       "11             0.953052  0.966667  0.959811    420.000000\n",
       "12             0.985075  0.956522  0.970588    690.000000\n",
       "13             1.000000  0.979167  0.989474    720.000000\n",
       "14             1.000000  0.996296  0.998145    270.000000\n",
       "15             0.985782  0.990476  0.988124    210.000000\n",
       "16             1.000000  0.893333  0.943662    150.000000\n",
       "17             1.000000  0.980556  0.990182    360.000000\n",
       "18             0.973913  0.861538  0.914286    390.000000\n",
       "19             0.846154  0.916667  0.880000     60.000000\n",
       "20             0.820000  0.911111  0.863158     90.000000\n",
       "21             0.987952  0.911111  0.947977     90.000000\n",
       "22             1.000000  0.950000  0.974359    120.000000\n",
       "23             0.864198  0.933333  0.897436    150.000000\n",
       "24             1.000000  0.955556  0.977273     90.000000\n",
       "25             0.923848  0.960417  0.941777    480.000000\n",
       "26             0.986207  0.794444  0.880000    180.000000\n",
       "27             0.951613  0.983333  0.967213     60.000000\n",
       "28             0.979592  0.960000  0.969697    150.000000\n",
       "29             1.000000  0.977778  0.988764     90.000000\n",
       "30             0.712575  0.793333  0.750789    150.000000\n",
       "31             0.981618  0.988889  0.985240    270.000000\n",
       "32             0.983607  1.000000  0.991736     60.000000\n",
       "33             0.921053  1.000000  0.958904    210.000000\n",
       "34             1.000000  0.975000  0.987342    120.000000\n",
       "35             1.000000  0.938462  0.968254    390.000000\n",
       "36             1.000000  0.941667  0.969957    120.000000\n",
       "37             0.983333  0.983333  0.983333     60.000000\n",
       "38             0.989796  0.984058  0.986919    690.000000\n",
       "39             1.000000  0.722222  0.838710     90.000000\n",
       "40             0.861702  0.900000  0.880435     90.000000\n",
       "41             0.963636  0.883333  0.921739     60.000000\n",
       "42             0.977778  0.977778  0.977778     90.000000\n",
       "accuracy       0.945527  0.945527  0.945527      0.945527\n",
       "macro avg      0.950055  0.924404  0.931984  12630.000000\n",
       "weighted avg   0.949823  0.945527  0.945246  12630.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_report=pd.DataFrame(test_report).T\n",
    "test_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd5739-cedd-4853-acd8-2c274ed931bc",
   "metadata": {},
   "source": [
    "### 4 Analysis of Classification Table\n",
    "\n",
    "#### Note: The analysis section contains analyses of different runs of the model. Due to the model's dynamic and random training and testing splits, the results of the model and hence the classification report change every time. It must be noted that even though these analyses may not relate to the current results and classification report of the model, they still represent the behavior of the model and help identify weaker classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "318db998-dfb0-4203-bb2d-843c1815e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report=test_report.reset_index().rename(columns={\n",
    "    \"index\" : \"Class Label\",\n",
    "    \"precision\" : \"Testing Precision\",\n",
    "    \"recall\" : \"Testing Recall\",\n",
    "    \"f1-score\" : \"Testing F1 Score\",\n",
    "    \"support\" : \"Testing Support\"\n",
    "})\n",
    "report=report.reset_index().rename(columns={\n",
    "    \"index\" : \"Class Label\",\n",
    "    \"precision\" : \"Training Precision\",\n",
    "    \"recall\":\"Training Recall\",\n",
    "    \"f1-score\":\"Training F1 Score\",\n",
    "    \"support\" : \"Training Support\"\n",
    "})\n",
    "report_df=pd.merge(test_report,report,on=\"Class Label\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c1b6d15-5563-4e49-b59d-683f412cc248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Label</th>\n",
       "      <th>Testing Precision</th>\n",
       "      <th>Testing Recall</th>\n",
       "      <th>Testing F1 Score</th>\n",
       "      <th>Testing Support</th>\n",
       "      <th>Training Precision</th>\n",
       "      <th>Training Recall</th>\n",
       "      <th>Training F1 Score</th>\n",
       "      <th>Training Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.923280</td>\n",
       "      <td>0.969444</td>\n",
       "      <td>0.945799</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>0.983945</td>\n",
       "      <td>0.979452</td>\n",
       "      <td>0.981693</td>\n",
       "      <td>438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.991465</td>\n",
       "      <td>0.929333</td>\n",
       "      <td>0.959394</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>0.990930</td>\n",
       "      <td>0.977629</td>\n",
       "      <td>0.984234</td>\n",
       "      <td>447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>0.872964</td>\n",
       "      <td>0.988930</td>\n",
       "      <td>0.927336</td>\n",
       "      <td>271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.989547</td>\n",
       "      <td>0.860606</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952719</td>\n",
       "      <td>0.975787</td>\n",
       "      <td>423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.907760</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.944402</td>\n",
       "      <td>630.000000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.964187</td>\n",
       "      <td>366.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.905797</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.911162</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>0.984962</td>\n",
       "      <td>0.919298</td>\n",
       "      <td>0.950998</td>\n",
       "      <td>285.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.813996</td>\n",
       "      <td>0.982222</td>\n",
       "      <td>0.890232</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>0.928328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962832</td>\n",
       "      <td>272.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.956697</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.992537</td>\n",
       "      <td>0.983364</td>\n",
       "      <td>268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.926657</td>\n",
       "      <td>0.995455</td>\n",
       "      <td>0.959825</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>0.970732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985149</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.953052</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.959811</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992620</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>0.992754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995516</td>\n",
       "      <td>0.997753</td>\n",
       "      <td>446.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>0.998145</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.993506</td>\n",
       "      <td>155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.985782</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.988124</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.995745</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980556</td>\n",
       "      <td>0.990182</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>219.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>0.976959</td>\n",
       "      <td>0.995305</td>\n",
       "      <td>0.986047</td>\n",
       "      <td>213.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.980892</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.947977</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.993464</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.923848</td>\n",
       "      <td>0.960417</td>\n",
       "      <td>0.941777</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969925</td>\n",
       "      <td>0.984733</td>\n",
       "      <td>133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.750789</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.952830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.981618</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.985240</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.993671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.986784</td>\n",
       "      <td>113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995392</td>\n",
       "      <td>0.997691</td>\n",
       "      <td>217.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.969957</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.984058</td>\n",
       "      <td>0.986919</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.998870</td>\n",
       "      <td>443.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.950055</td>\n",
       "      <td>0.924404</td>\n",
       "      <td>0.931984</td>\n",
       "      <td>12630.000000</td>\n",
       "      <td>0.989038</td>\n",
       "      <td>0.984069</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>7842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.949823</td>\n",
       "      <td>0.945527</td>\n",
       "      <td>0.945246</td>\n",
       "      <td>12630.000000</td>\n",
       "      <td>0.985008</td>\n",
       "      <td>0.984188</td>\n",
       "      <td>0.984276</td>\n",
       "      <td>7842.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class Label  Testing Precision  Testing Recall  Testing F1 Score  \\\n",
       "0              0           1.000000        0.383333          0.554217   \n",
       "1              1           0.923280        0.969444          0.945799   \n",
       "2              2           0.991465        0.929333          0.959394   \n",
       "3              3           0.814607        0.966667          0.884146   \n",
       "4              4           0.989547        0.860606          0.920583   \n",
       "5              5           0.907760        0.984127          0.944402   \n",
       "6              6           0.992063        0.833333          0.905797   \n",
       "7              7           0.934579        0.888889          0.911162   \n",
       "8              8           0.813996        0.982222          0.890232   \n",
       "9              9           0.925926        0.989583          0.956697   \n",
       "10            10           0.926657        0.995455          0.959825   \n",
       "11            11           0.953052        0.966667          0.959811   \n",
       "12            12           0.985075        0.956522          0.970588   \n",
       "13            13           1.000000        0.979167          0.989474   \n",
       "14            14           1.000000        0.996296          0.998145   \n",
       "15            15           0.985782        0.990476          0.988124   \n",
       "16            16           1.000000        0.893333          0.943662   \n",
       "17            17           1.000000        0.980556          0.990182   \n",
       "18            18           0.973913        0.861538          0.914286   \n",
       "19            19           0.846154        0.916667          0.880000   \n",
       "20            20           0.820000        0.911111          0.863158   \n",
       "21            21           0.987952        0.911111          0.947977   \n",
       "22            22           1.000000        0.950000          0.974359   \n",
       "23            23           0.864198        0.933333          0.897436   \n",
       "24            24           1.000000        0.955556          0.977273   \n",
       "25            25           0.923848        0.960417          0.941777   \n",
       "26            26           0.986207        0.794444          0.880000   \n",
       "27            27           0.951613        0.983333          0.967213   \n",
       "28            28           0.979592        0.960000          0.969697   \n",
       "29            29           1.000000        0.977778          0.988764   \n",
       "30            30           0.712575        0.793333          0.750789   \n",
       "31            31           0.981618        0.988889          0.985240   \n",
       "32            32           0.983607        1.000000          0.991736   \n",
       "33            33           0.921053        1.000000          0.958904   \n",
       "34            34           1.000000        0.975000          0.987342   \n",
       "35            35           1.000000        0.938462          0.968254   \n",
       "36            36           1.000000        0.941667          0.969957   \n",
       "37            37           0.983333        0.983333          0.983333   \n",
       "38            38           0.989796        0.984058          0.986919   \n",
       "39            39           1.000000        0.722222          0.838710   \n",
       "40            40           0.861702        0.900000          0.880435   \n",
       "41            41           0.963636        0.883333          0.921739   \n",
       "42            42           0.977778        0.977778          0.977778   \n",
       "43      accuracy           0.945527        0.945527          0.945527   \n",
       "44     macro avg           0.950055        0.924404          0.931984   \n",
       "45  weighted avg           0.949823        0.945527          0.945246   \n",
       "\n",
       "    Testing Support  Training Precision  Training Recall  Training F1 Score  \\\n",
       "0         60.000000            1.000000         0.888889           0.941176   \n",
       "1        720.000000            0.983945         0.979452           0.981693   \n",
       "2        750.000000            0.990930         0.977629           0.984234   \n",
       "3        450.000000            0.872964         0.988930           0.927336   \n",
       "4        660.000000            1.000000         0.952719           0.975787   \n",
       "5        630.000000            0.972222         0.956284           0.964187   \n",
       "6        150.000000            1.000000         1.000000           1.000000   \n",
       "7        450.000000            0.984962         0.919298           0.950998   \n",
       "8        450.000000            0.928328         1.000000           0.962832   \n",
       "9        480.000000            0.974359         0.992537           0.983364   \n",
       "10       660.000000            0.970732         1.000000           0.985149   \n",
       "11       420.000000            1.000000         0.992620           0.996296   \n",
       "12       690.000000            0.992754         1.000000           0.996364   \n",
       "13       720.000000            1.000000         0.995516           0.997753   \n",
       "14       270.000000            1.000000         0.987097           0.993506   \n",
       "15       210.000000            1.000000         0.991525           0.995745   \n",
       "16       150.000000            1.000000         0.938776           0.968421   \n",
       "17       360.000000            0.995434         0.995434           0.995434   \n",
       "18       390.000000            0.976959         0.995305           0.986047   \n",
       "19        60.000000            0.976190         1.000000           0.987952   \n",
       "20        90.000000            1.000000         0.962500           0.980892   \n",
       "21        90.000000            1.000000         1.000000           1.000000   \n",
       "22       120.000000            1.000000         0.987013           0.993464   \n",
       "23       150.000000            1.000000         0.969697           0.984615   \n",
       "24        90.000000            1.000000         0.970149           0.984848   \n",
       "25       480.000000            0.996865         0.996865           0.996865   \n",
       "26       180.000000            1.000000         0.969925           0.984733   \n",
       "27        60.000000            1.000000         1.000000           1.000000   \n",
       "28       150.000000            1.000000         1.000000           1.000000   \n",
       "29        90.000000            0.983051         1.000000           0.991453   \n",
       "30       150.000000            0.952830         1.000000           0.975845   \n",
       "31       270.000000            0.993671         1.000000           0.996825   \n",
       "32        60.000000            1.000000         1.000000           1.000000   \n",
       "33       210.000000            0.982456         0.991150           0.986784   \n",
       "34       120.000000            1.000000         0.958333           0.978723   \n",
       "35       390.000000            1.000000         0.995392           0.997691   \n",
       "36       120.000000            1.000000         1.000000           1.000000   \n",
       "37        60.000000            1.000000         0.979592           0.989691   \n",
       "38       690.000000            1.000000         0.997743           0.998870   \n",
       "39        90.000000            1.000000         0.984615           0.992248   \n",
       "40        90.000000            1.000000         1.000000           1.000000   \n",
       "41        60.000000            1.000000         1.000000           1.000000   \n",
       "42        90.000000            1.000000         1.000000           1.000000   \n",
       "43         0.945527            0.984188         0.984188           0.984188   \n",
       "44     12630.000000            0.989038         0.984069           0.986228   \n",
       "45     12630.000000            0.985008         0.984188           0.984276   \n",
       "\n",
       "    Training Support  \n",
       "0          36.000000  \n",
       "1         438.000000  \n",
       "2         447.000000  \n",
       "3         271.000000  \n",
       "4         423.000000  \n",
       "5         366.000000  \n",
       "6          93.000000  \n",
       "7         285.000000  \n",
       "8         272.000000  \n",
       "9         268.000000  \n",
       "10        398.000000  \n",
       "11        271.000000  \n",
       "12        411.000000  \n",
       "13        446.000000  \n",
       "14        155.000000  \n",
       "15        118.000000  \n",
       "16         98.000000  \n",
       "17        219.000000  \n",
       "18        213.000000  \n",
       "19         41.000000  \n",
       "20         80.000000  \n",
       "21         69.000000  \n",
       "22         77.000000  \n",
       "23         99.000000  \n",
       "24         67.000000  \n",
       "25        319.000000  \n",
       "26        133.000000  \n",
       "27         47.000000  \n",
       "28        125.000000  \n",
       "29         58.000000  \n",
       "30        101.000000  \n",
       "31        157.000000  \n",
       "32         39.000000  \n",
       "33        113.000000  \n",
       "34         72.000000  \n",
       "35        217.000000  \n",
       "36         73.000000  \n",
       "37         49.000000  \n",
       "38        443.000000  \n",
       "39         65.000000  \n",
       "40         71.000000  \n",
       "41         54.000000  \n",
       "42         45.000000  \n",
       "43          0.984188  \n",
       "44       7842.000000  \n",
       "45       7842.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cf026-3a97-48db-abc0-3c0e161467c4",
   "metadata": {},
   "source": [
    "    ##### First Run: \n",
    "                                        let us consider class 0 as a standard to understand the performance of the model. Precision =1 of the model for class 0 implies that all images predicted for class 0 were correct; recall=0.83 implies that the model identified 83 % of the total class 0 images correctly, but it also means that the model missed 17% of the total images i.e. 17% of 60 which is 14.\n",
    "\n",
    "                                        Let us compare this classification report to the training classification report to reveal more about the model.\n",
    "\n",
    "                                        \n",
    "    By comparing the recall and precision of each class in the testing and training phases, we can find the classes with which the model struggles.\n",
    "    1. Class 0 had a recall drop from 97 % in training to 86% in testing which shows that the model may be overfitted for this class. When considering the support for this class, it can be reasoned that the cause of overfitting is the small size of training data for this class. \n",
    "    2. Similarly class 6 in the training phase had a score of 1 i.e. 100% in precision and recall, but in the testing phase precision remained approximately the same but recall dropped by 10%. This implies that the model only predicts images of class 6 when it is confident.\n",
    "    3. There is a significant drop in precision for images of class 20. The model had a high precision of 98% in the training phase, but in the testing phase, precision dropped to 67%. A ~30% drop in precision is significant. However, this does not mean that the model could not identify images from class 20 as it has a high recall of 97% in the testing phase. This implies that the model was able to classify 97% of the images of class 20 correctly. The drop in precision is because many images not belonging to class 20 were predicted as of class 20, this means that when the model is not confident about the classification of an image then it predicts it as of class 20. The cause of this may be because of the fact that support for class 20 in the training phase(80) is less than the support in the testing phase(90). We can safely conclude that the model needs relearning for class 20 as the model is overpredicting class 20 which cannot be justified by less training data.  \n",
    "    4.  Class 21 had a score of 100% in precision and recall in training data, but even though precision remained the same, recall dropped by 17%. The cause of this is the difference in support during the training and testing phases of the model. In the training phase, support was 69, but in the testing phase, it was 90. This implies that the model is confident in its prediction for class 21, but misses data due to less training data.\n",
    "    5. Class 26 suffers the same fate as class 21. \n",
    "    6. The model is very confident about its prediction for class 27 which is shown by its high precision of 100% in both the testing and training phases, but because of this the recall of the model in the testing phase dropped by ~29%. A standard case of overfitting, the model is not generalizing well. \n",
    "    7. The model seems to miss images of class 30 indicating the recall drop of ~14%. Again, like classes 21, 26 and 27, support of the testing phase is more than that of the training phase.\n",
    "    8. Classes 40 and 42 also have the same problem as classes 21,26, 27 and 30. Both classes had a recall drop of 23%. Support of the the testing phase is less than the support of the training phase especially in the case of class 42 where the support of the testing phase is twice the size of the support of the training phase. \n",
    "\n",
    "    Conclusion: We can classify the problems identified in the analysis in the following. \n",
    "            Less Training data problem: Classes 0, 6, 21, 26, 27, 30, 40 and 42. \n",
    "            Learning Problem: Class 20\n",
    "        Note: Class 20 also has less training data when compared to the testing data, however, the difference was only 10 which can not justify the drop in precision.\n",
    "        Additionally, the dataset is imbalanced as many classes have support less than 100 which makes them vulnerable to performance loss particularly recall drop. \n",
    "\n",
    "\n",
    "    ##### Second Run: \n",
    "    The weaker classes for this run were classes 0,19,18,20,26,30,39,40 and 41.\n",
    "    1. Class 0 had a very low recall of ~38% which shows that this class is a key problem class for the model as it was also present in the previous run. However, this time model performed even worse for this class. \n",
    "    2. Class 19 has a precision problem, but has high recall which implies that the model has trouble identifying this class. However, it is too soon to say anything for certain. Perhaps the precision arises from other classes. \n",
    "    3. Class 18 has a drop in recall during the testing phase. However, as this class did not appear in the previous run, we cannot say anything for certain yet. \n",
    "    4. Class 20 in this run has a precision drop of 18% and a recall drop of only 5% which is similar to its state in the previous run, but milder this time. We can safely assume this class is a problem class for the model.\n",
    "    5. Class 26 was present in the list of problem classes in the previous run as well. It had the same problem in the previous run. It has a high precision, but recall seems to have an approximate drop of 17% in both runs. \n",
    "    6. Class 30 has a precision drop of 24% and a recall drop of 21%. This indicates that the model has a problem learning this class, but again it may be due to other problem classes particularly 40 and 42 which performed well in this run somehow. \n",
    "    7. Class 39 has a recall drop of 26%, but a precision of 100% which indicates that the model is predicting images of class 39  only when it is confident and many images of class 39 are being misclassified probably as class 30 which may explain its precision drop. \n",
    "    8. Class 40 has a 14% precision drop and a 10% recall drop which is better than its state in the previous run where it had a 22% recall drop. \n",
    "    9. Class 41 has a 12% recall drop and a good precision. It has a very small size of support though.\n",
    "    10. Class 42 has performed very well in this run. However, it has a small size of support and had a large 30% drop in recall in the previous run.\n",
    "\n",
    "    Conclusion:\n",
    "    Classes to still consider in future runs: 18,19,39,41,26\n",
    "    Problem classes to augment: 0,40,42,20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b665e0-efd4-408f-95aa-415721b6a702",
   "metadata": {},
   "source": [
    "### 4. Augementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865000d4-2e3f-42f5-b47d-4e3da64628fc",
   "metadata": {},
   "source": [
    "    Based on analyses of pre-augmented runs, the identified problem classes were grouped together and were augmented to test the model performance. The summary of these post-augmentation runs is as following: \n",
    "| Augmented Classes | Accuracy (%) | Notes                                                       |\n",
    "| ----------------- | ------------ | ----------------------------------------------------------- |\n",
    "| 0, 20, 27, 40, 42 | **97.48**    | Best raw accuracy; slight overfit in a few classes          |\n",
    "| **0, 40, 30, 23** | **97.42**    | Most balanced overall; all but 2 classes above ~90% P/R      |\n",
    "| 0, 20, 42, 40     | 95.94        | Class 20 overfit, others average                            |\n",
    "| 0, 20, 42, 40, 41 | 95.93        | Class 41 recall drop                                        |\n",
    "| 0, 42, 40, 41, 27 | 96.69        | Class 42 precision drop; class 20 okay without augmentation |\n",
    "| 0, 41, 40         | 96.10        | Class 19/20/41 weak                                         |\n",
    "| 0, 40             | 95.66    | Similar to 0, 41,40 run, but weaker in classes 23 and 30        |\n",
    "\n",
    "    Summary of class based reports for the runs:\n",
    "| Run (Augmented Classes) | Accuracy   | Class | Precision | Recall | Notes                                    |\n",
    "| ----------------------- | ---------- | ----- | --------- | ------ | ---------------------------------------- |\n",
    "| `0, 42, 41, 40, 27`     | 96.70%     | 0     | 0.933     | 0.982  | Good gain, not overfitting               |\n",
    "|                         |            | 20    | 0.978     | 0.759  | NOT augmented — recall decent            |\n",
    "|                         |            | 27    | 0.717     | 0.935  | Overfitting suspected                    |\n",
    "|                         |            | 40    | 0.978     | 0.846  | Solid                                    |\n",
    "|                         |            | 42    | 0.733     | 0.985  | High recall, low precision (overfitting) |\n",
    "| `0, 20, 42, 40`         | 95.94%     | 0     | 0.917     | 1.000  | Boosted recall, solid                    |\n",
    "|                         |            | 20    | 0.944     | 0.780  | Helped recall, precision still meh       |\n",
    "|                         |            | 27    | 0.800     | 0.873  | NOT augmented                            |\n",
    "|                         |            | 40    | 0.889     | 0.964  | High recall gain                         |\n",
    "|                         |            | 42    | 0.833     | 0.949  | Slight overfitting                       |\n",
    "| `20, 27, 40, 42`        | 97.48%     | 0     | 1.000     | 1.000  | NOT augmented — excellent                |\n",
    "|                         |            | 20    | 0.967     | 0.664  | Over-augmented, high FP (low precision)  |\n",
    "|                         |            | 27    | 0.717     | 0.977  | Strong recall, weak precision (overfit)  |\n",
    "|                         |            | 40    | 0.900     | 0.976  | Excellent                                |\n",
    "|                         |            | 42    | 0.878     | 0.963  | Still recall-dominant                    |\n",
    "| `0, 41, 40`             | 96.10%     | 0     | 0.900     | 1.000  | Consistent gain                          |\n",
    "|                         |            | 20    | 0.989     | 0.669  | NOT augmented — poor recall              |\n",
    "|                         |            | 41    | 1.000     | 0.732  | Augmented, recall dropped                |\n",
    "|                         |            | 40    | 0.956     | 0.851  | Slight drop in recall                    |\n",
    "| `0, 40, 30, 23`         | 97.42%   | 0     | 0.933     | 1.000  | Consistent; good result                  |\n",
    "|                         |          | 20    | 0.978     | 0.779  | Still a weak link (recall drops)         |\n",
    "|                         |          | 23    | 0.980     | 0.955  | Excellent gain                           |\n",
    "|                         |          | 30    | 0.893     | 0.905  | Huge win for a known problem class       |\n",
    "|                         |          | 40    | 0.911     | 0.891  | Stable & better than many other runs     |\n",
    "|                         |          | 41    | 0.767     | 0.939  | Overfit — very low precision             |\n",
    "\n",
    "    Summary of class weights in augmentation runs:\n",
    "| Class  | Consistently Benefits from Augmentation | Notes                                                            |\n",
    "| ------ | --------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **0**  | ✅ Yes                                   | Stable gains in all runs                                         |\n",
    "| **20** | ❌ No                                    | Always erratic, usually hurts recall or precision                |\n",
    "| **23** | ✅ Yes                                   | First time tested, gave solid results                            |\n",
    "| **30** | ✅ Yes                                   | Former weak class, now above 90% for both P/R                    |\n",
    "| **40** | ✅ Yes                                   | Excellent in nearly every combo                                  |\n",
    "| **41** | ❌ No                                    | Repeated overfit — recall boosts at cost of precision            |\n",
    "| **42** | ❌ No                                    | Repeated overfit — recall boosts at cost of precision            |\n",
    "\n",
    "\n",
    "    Based on these results, the best run (0,40,30,23) was selected to be displayed because it has a high accuracy and also has the most fairly balanced classes. \n",
    "\n",
    "    Note:- The results may differ as the train-test split is different each time the whole notebook is executed. The results shown here may not be exactly reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "524031f4-fb28-40b7-8bfd-94e2b74c2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libaray to augment images\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45934c7a-6db7-4e37-a1bc-80059623b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An instance of transforms; using Compose to apply a set operations together\n",
    "transform=transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), #Flips images horizontally randomly\n",
    "    transforms.RandomRotation(10), #Rotates images by 10 degress randomly\n",
    "    transforms.ColorJitter(brightness=0.2,contrast=0.2) #changes brightness and contrast of images.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26e7a119-f6a1-4502-a801-9f688a95ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "class_id=[]\n",
    "for i in train_df[\"Path\"]:\n",
    "    img_path=os.path.join(base_directory,i)\n",
    "    img=Image.open(img_path)\n",
    "    ID=train_df.loc[train_df[\"Path\"]==i,\"ClassId\"].values[0]\n",
    "    img=img.resize((32,32)) #resize\n",
    "    if ID in [0,40,30,23]:\n",
    "        for _ in range(1):  # 1x augmentation\n",
    "            aug_img = transform(img) # increases support for weaker classes\n",
    "            images.append(np.array(aug_img)/255)\n",
    "            class_id.append(ID)\n",
    "    img=np.array(img)/255 #Normalization\n",
    "    images.append(img)\n",
    "    class_id.append(ID)\n",
    "train_x,valid_x,train_y,valid_y=train_test_split(images,class_id,test_size=0.2,random_state=0) #train test split\n",
    "train_x=np.array(train_x) # convert to array for faster tensor conversion\n",
    "train_y=np.array(train_y)\n",
    "valid_x=np.array(valid_x)\n",
    "valid_y=np.array(valid_y)\n",
    "tensor_train_x=torch.tensor(train_x,dtype=torch.float32) #convert to tensor\n",
    "tensor_train_y=torch.tensor(train_y,dtype=torch.int64)\n",
    "tensor_valid_x=torch.tensor(valid_x,dtype=torch.float32)\n",
    "tensor_valid_y=torch.tensor(valid_y,dtype=torch.int64)\n",
    "tensor_train_x=tensor_train_x.permute(0,3,1,2) #correct the channel position\n",
    "tensor_valid_x=tensor_valid_x.permute(0,3,1,2)\n",
    "Augmented_train_dataset=TensorDataset(tensor_train_x,tensor_train_y) # to tensor datasets\n",
    "Augmented_valid_dataset=TensorDataset(tensor_valid_x,tensor_valid_y)\n",
    "Augmented_train_loader=DataLoader(Augmented_train_dataset,batch_size=64,shuffle=True) #create data loaders\n",
    "Augmented_valid_loader=DataLoader(Augmented_valid_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9165052d-f1c4-4469-a462-10443c256f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Augmented_CNN=get_cnn_model()\n",
    "aug_accuracy,aug_training_time,predictedlabels,truelabels=train_model(Augmented_train_loader,Augmented_valid_loader,Augmented_CNN,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e19fde9-e448-4996-994e-a8460ebfd4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>0.994595</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993197</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>441.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993576</td>\n",
       "      <td>0.996778</td>\n",
       "      <td>467.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>0.996416</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997602</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.991781</td>\n",
       "      <td>0.983696</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.996324</td>\n",
       "      <td>0.981884</td>\n",
       "      <td>0.989051</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.988189</td>\n",
       "      <td>0.996032</td>\n",
       "      <td>0.992095</td>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.990228</td>\n",
       "      <td>0.993464</td>\n",
       "      <td>0.991843</td>\n",
       "      <td>306.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.997596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998797</td>\n",
       "      <td>415.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.996032</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.988189</td>\n",
       "      <td>256.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>161.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.992126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996047</td>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.988372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.996094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998043</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.990909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>327.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.983193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.985366</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.988764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.992593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996283</td>\n",
       "      <td>134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.996063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998028</td>\n",
       "      <td>253.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>409.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.995459</td>\n",
       "      <td>0.995459</td>\n",
       "      <td>0.995459</td>\n",
       "      <td>0.995459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.995812</td>\n",
       "      <td>0.994721</td>\n",
       "      <td>0.995240</td>\n",
       "      <td>8148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.995496</td>\n",
       "      <td>0.995459</td>\n",
       "      <td>0.995459</td>\n",
       "      <td>8148.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              1.000000  0.989247  0.994595    93.000000\n",
       "1              0.993197  0.993197  0.993197   441.000000\n",
       "2              1.000000  0.993576  0.996778   467.000000\n",
       "3              1.000000  0.992857  0.996416   280.000000\n",
       "4              1.000000  0.997602  0.998800   417.000000\n",
       "5              0.975741  0.991781  0.983696   365.000000\n",
       "6              1.000000  1.000000  1.000000    96.000000\n",
       "7              0.996324  0.981884  0.989051   276.000000\n",
       "8              0.988189  0.996032  0.992095   252.000000\n",
       "9              0.990228  0.993464  0.991843   306.000000\n",
       "10             0.997596  1.000000  0.998797   415.000000\n",
       "11             0.996032  0.980469  0.988189   256.000000\n",
       "12             1.000000  1.000000  1.000000   390.000000\n",
       "13             1.000000  1.000000  1.000000   416.000000\n",
       "14             1.000000  1.000000  1.000000   161.000000\n",
       "15             0.992126  1.000000  0.996047   126.000000\n",
       "16             0.988372  1.000000  0.994152    85.000000\n",
       "17             1.000000  1.000000  1.000000   214.000000\n",
       "18             0.996094  1.000000  0.998043   255.000000\n",
       "19             1.000000  0.975000  0.987342    40.000000\n",
       "20             0.959459  0.972603  0.965986    73.000000\n",
       "21             1.000000  1.000000  1.000000    59.000000\n",
       "22             1.000000  1.000000  1.000000    76.000000\n",
       "23             0.995025  0.990099  0.992556   202.000000\n",
       "24             1.000000  1.000000  1.000000    56.000000\n",
       "25             0.990909  1.000000  0.995434   327.000000\n",
       "26             0.983193  1.000000  0.991525   117.000000\n",
       "27             1.000000  1.000000  1.000000    49.000000\n",
       "28             1.000000  0.971154  0.985366   104.000000\n",
       "29             1.000000  1.000000  1.000000    54.000000\n",
       "30             0.988764  1.000000  0.994350   176.000000\n",
       "31             0.992593  1.000000  0.996283   134.000000\n",
       "32             1.000000  1.000000  1.000000    40.000000\n",
       "33             1.000000  1.000000  1.000000   156.000000\n",
       "34             1.000000  0.977273  0.988506    88.000000\n",
       "35             0.996063  1.000000  0.998028   253.000000\n",
       "36             1.000000  1.000000  1.000000    77.000000\n",
       "37             1.000000  0.976744  0.988235    43.000000\n",
       "38             1.000000  1.000000  1.000000   409.000000\n",
       "39             1.000000  1.000000  1.000000    62.000000\n",
       "40             1.000000  1.000000  1.000000   157.000000\n",
       "41             1.000000  1.000000  1.000000    38.000000\n",
       "42             1.000000  1.000000  1.000000    47.000000\n",
       "accuracy       0.995459  0.995459  0.995459     0.995459\n",
       "macro avg      0.995812  0.994721  0.995240  8148.000000\n",
       "weighted avg   0.995496  0.995459  0.995459  8148.000000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_report=classification_report(truelabels,predictedlabels,output_dict=True)\n",
    "aug_df=pd.DataFrame(aug_report).T\n",
    "aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b1cc3694-e6c3-4594-882e-695cb63dd08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time of Model on Augmented data:  30.80717806418737 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Training Time of Model on Augmented data: \", aug_training_time/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb0dba-24cd-4066-b247-326f800347d6",
   "metadata": {},
   "source": [
    "##### Testing the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c910955d-07a9-46c5-879e-ca50d2b8abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug_test_accuracy,aug_predicted_label,aug_true_label=model_test(Augmented_CNN,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "69efaa31-7153-4f8b-9fe8-3473ad20374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.980556</td>\n",
       "      <td>0.976487</td>\n",
       "      <td>0.978517</td>\n",
       "      <td>723.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.992032</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>753.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.962222</td>\n",
       "      <td>0.997696</td>\n",
       "      <td>0.979638</td>\n",
       "      <td>434.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.974242</td>\n",
       "      <td>0.972769</td>\n",
       "      <td>0.973505</td>\n",
       "      <td>661.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.909621</td>\n",
       "      <td>0.948328</td>\n",
       "      <td>686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.826667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905109</td>\n",
       "      <td>124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.924444</td>\n",
       "      <td>0.978824</td>\n",
       "      <td>0.950857</td>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.971111</td>\n",
       "      <td>0.929787</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.957916</td>\n",
       "      <td>0.976507</td>\n",
       "      <td>499.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.995455</td>\n",
       "      <td>0.980597</td>\n",
       "      <td>0.987970</td>\n",
       "      <td>670.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.957944</td>\n",
       "      <td>0.966981</td>\n",
       "      <td>428.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.986957</td>\n",
       "      <td>0.998534</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>682.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.997222</td>\n",
       "      <td>0.994460</td>\n",
       "      <td>0.995839</td>\n",
       "      <td>722.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.995261</td>\n",
       "      <td>212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995816</td>\n",
       "      <td>357.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.971279</td>\n",
       "      <td>376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.866995</td>\n",
       "      <td>113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.982609</td>\n",
       "      <td>0.961702</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.967105</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.974411</td>\n",
       "      <td>497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.827778</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.971751</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.899329</td>\n",
       "      <td>148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.996226</td>\n",
       "      <td>0.986916</td>\n",
       "      <td>265.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.995238</td>\n",
       "      <td>0.985849</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.994872</td>\n",
       "      <td>0.984772</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.985507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992701</td>\n",
       "      <td>680.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.922222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959538</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.974188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.959461</td>\n",
       "      <td>0.970299</td>\n",
       "      <td>0.963721</td>\n",
       "      <td>12630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.975398</td>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>12630.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.933333  1.000000  0.965517     56.000000\n",
       "1              0.980556  0.976487  0.978517    723.000000\n",
       "2              0.996000  0.992032  0.994012    753.000000\n",
       "3              0.962222  0.997696  0.979638    434.000000\n",
       "4              0.974242  0.972769  0.973505    661.000000\n",
       "5              0.990476  0.909621  0.948328    686.000000\n",
       "6              0.826667  1.000000  0.905109    124.000000\n",
       "7              0.924444  0.978824  0.950857    425.000000\n",
       "8              0.971111  0.929787  0.950000    470.000000\n",
       "9              0.995833  0.957916  0.976507    499.000000\n",
       "10             0.995455  0.980597  0.987970    670.000000\n",
       "11             0.976190  0.957944  0.966981    428.000000\n",
       "12             0.986957  0.998534  0.992711    682.000000\n",
       "13             0.997222  0.994460  0.995839    722.000000\n",
       "14             1.000000  1.000000  1.000000    270.000000\n",
       "15             1.000000  0.990566  0.995261    212.000000\n",
       "16             0.973333  1.000000  0.986486    146.000000\n",
       "17             0.991667  1.000000  0.995816    357.000000\n",
       "18             0.953846  0.989362  0.971279    376.000000\n",
       "19             1.000000  0.869565  0.930233     69.000000\n",
       "20             0.977778  0.778761  0.866995    113.000000\n",
       "21             0.888889  0.987654  0.935673     81.000000\n",
       "22             0.941667  0.982609  0.961702    115.000000\n",
       "23             0.980000  0.954545  0.967105    154.000000\n",
       "24             0.955556  1.000000  0.977273     86.000000\n",
       "25             0.991667  0.957746  0.974411    497.000000\n",
       "26             0.827778  0.955128  0.886905    156.000000\n",
       "27             0.983333  0.983333  0.983333     60.000000\n",
       "28             0.986667  0.986667  0.986667    150.000000\n",
       "29             0.955556  0.988506  0.971751     87.000000\n",
       "30             0.893333  0.905405  0.899329    148.000000\n",
       "31             0.977778  0.996226  0.986916    265.000000\n",
       "32             1.000000  0.983607  0.991736     61.000000\n",
       "33             0.995238  0.985849  0.990521    212.000000\n",
       "34             0.983333  0.983333  0.983333    120.000000\n",
       "35             0.994872  0.984772  0.989796    394.000000\n",
       "36             0.975000  1.000000  0.987342    117.000000\n",
       "37             0.933333  0.982456  0.957265     57.000000\n",
       "38             0.985507  1.000000  0.992701    680.000000\n",
       "39             1.000000  1.000000  1.000000     90.000000\n",
       "40             0.911111  0.891304  0.901099     92.000000\n",
       "41             0.766667  0.938776  0.844037     49.000000\n",
       "42             0.922222  1.000000  0.959538     83.000000\n",
       "accuracy       0.974188  0.974188  0.974188      0.974188\n",
       "macro avg      0.959461  0.970299  0.963721  12630.000000\n",
       "weighted avg   0.975398  0.974188  0.974247  12630.000000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_testreport=classification_report(aug_predicted_label,aug_true_label,output_dict=True)\n",
    "aug_testreport_df=pd.DataFrame(aug_testreport).T\n",
    "aug_testreport_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529e550-c3ca-42c7-9c69-cfd9dcb66236",
   "metadata": {},
   "source": [
    "### 4. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e5d93-3aa5-40eb-87bf-5283c7216a4a",
   "metadata": {},
   "source": [
    "    The model achieves an accuracy between 98-99% on training data and an accuracy of 96-97% on testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f62706c5-0b1f-412a-86a9-3b15ec4b9fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Session</th>\n",
       "      <td>99.545901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing Session</th>\n",
       "      <td>97.418844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Accuracy\n",
       "Training Session  99.545901\n",
       "Testing Session   97.418844"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df=pd.DataFrame({\n",
    "    \"Accuracy\":[aug_accuracy,Aug_test_accuracy]\n",
    "},index=[\"Training Session\",\"Testing Session\"])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "acda22e0-7dd8-4da9-9dae-9aa008dddeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# barplot\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(result_df[\"Accuracy\"])\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ec870-badc-46b7-996d-f53bb8e49122",
   "metadata": {},
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d085e85-37a6-4869-bcaf-3c2f166fcd7c",
   "metadata": {},
   "source": [
    "#### Extra 1: Model Version History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d448b-813d-4212-bcc0-5b189690910f",
   "metadata": {},
   "source": [
    "    Version 1: Initial Model\n",
    "    The model architecture consisted of two convolutional layers followed by Maxpooling and two linear layers. The model performed well, achieving 98% accuracy on training data and 91% accuracy on testing data. The model has a training time of ~512 seconds. However, the model's accuracy was meeting the set goal and it was assumed that the model could not learn enough with 4 layers.  \n",
    "    \n",
    "    Version 2: Deeper Model\n",
    "    The second version of the model added another convolutional layer to improve accuracy. However, the accuracy remained unchanged while the training time tripled (~1600 seconds). Classification Report was added to identify the performance issues better and it showed clear signs of overfitting. \n",
    "    \n",
    "    Version 3: Generalization and Optimization\n",
    "    This version focused on reducing the overfitting and training time of the model. Two key changes were made:\n",
    "        a)Batch Normalization was added to normalize the output of each convolutional layer to reduce the load on the next layer. This improved the training speed of the model. \n",
    "        b) Dropout was added before the final linear layer which combined the learning of all three hidden layers. Dropout disabled(dropped) 30% of the neurons of the linear layer and reduced overfitting significantly. \n",
    "        This version of the model saw significant improvements in : \n",
    "        a) Accuracy of training and testing improved to 99% and 97% respectively. \n",
    "        b) Training time dropped to ~1000. \n",
    "    \n",
    "    Version 4: Future Version\n",
    "    A future version will include class-specific data augmentation to improve performance on underperforming classes, as identified through precision-recall analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4852a-6d00-45f0-b062-9a40410ab957",
   "metadata": {},
   "source": [
    "#### Extra 2: CPU vs GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58d8db-ba59-4c47-afe6-1baedf3ee6c5",
   "metadata": {},
   "source": [
    "    The following function can be used to compare model performance on GPU and CPU in terms of training time, latency, and throughput. However, these comparisons were not conducted in this project due to hardware and time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20f8515d-953e-4252-a247-d642e6b65f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train model\n",
    "def train_model(device,train_loader,valid_loader,CNN_model):\n",
    "    CNN_model.to(device) #transfer to given device\n",
    "    #train the model\n",
    "    #define loss function\n",
    "    loss_func=nn.CrossEntropyLoss()\n",
    "    #optimizer\n",
    "    optimizer=Adam(CNN_model.parameters())\n",
    "    epoch=3\n",
    "    total_img=0\n",
    "    total_validation_time=0\n",
    "    total_training_time=0\n",
    "    for i in range(epoch):\n",
    "        #training\n",
    "        CNN_model.train()\n",
    "        true_labels=[]\n",
    "        predicted_labels=[]\n",
    "        start_time=time.time()\n",
    "        for images,classID in train_loader:\n",
    "            images=images.to(device) #transfer to given device\n",
    "            classID=classID.to(device) #transfer to given device\n",
    "            optimizer.zero_grad()\n",
    "            outputs=CNN_model(images)  #predicts\n",
    "            loss=loss_func(outputs,classID) #calculates loss\n",
    "            loss.backward() #learns from the made predictions\n",
    "            optimizer.step() #optimized learning by Adam (updates weights)\n",
    "        \n",
    "        #Validating\n",
    "        CNN_model.eval()\n",
    "        correct=0\n",
    "        total=0\n",
    "        validation_start_time=time.time()\n",
    "        with torch.no_grad():\n",
    "            for images,classid in valid_loader:\n",
    "                images=images.to(device) #transfer to given device\n",
    "                classid=classid.to(device) #transfer to given device\n",
    "                outputs=CNN_model(images)\n",
    "                _,predicted=torch.max(outputs,1)\n",
    "                predicted_labels.extend(predicted.tolist())\n",
    "                true_labels.extend(classid.tolist())\n",
    "                correct+=(predicted==classid).sum().item()\n",
    "                total+=classid.size(0)\n",
    "        accuracy=(correct/total)*100\n",
    "        end_time=time.time()\n",
    "        validation_end_time=time.time()\n",
    "        total_img+=total\n",
    "        total_validation_time+=(end_time-validation_start_time)\n",
    "        total_training_time+=(end_time-start_time)\n",
    "\n",
    "    latency=total_validation_time/total_img\n",
    "    throughput=1/latency\n",
    "    return accuracy,total_training_time,latency,throughput,predicted_labels,true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2bde6-5d33-4a21-8186-7a9e0594cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on CPU\n",
    "CPU_CNN=get_cnn_model()\n",
    "device = \"cpu\"\n",
    "accuracy_cpu,total_training_time_cpu,latency_cpu,throughput_cpu,_,_=train_model(device,train_loader,valid_loader,CPU_CNN)\n",
    "# on GPU\n",
    "GPU_CNN=get_cnn_model()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "accuracy_gpu,total_training_time_gpu,latency_gpu,throughput_gpu,_,_=train_model(device,train_loader,valid_loader,GPU_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790997c-997f-4925-a266-fdcc90987f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df=pd.DataFrame({\n",
    "    \"CPU\": [accuracy_cpu,total_training_time_cpu,latency_cpu,throughput_cpu],\n",
    "    \"GPU\": [accuracy_gpu,total_training_time_gpu,latency_gpu,throughput_gpu]\n",
    "},index=[\"Accuracy\",\"Total_Training_Time\",\"Latency\",\"Throughput\"])\n",
    "print(compare_df.T)\n",
    "compare_df=compare_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f67f4-d9eb-40d0-8f60-a878815c814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize\n",
    "x=[\"Accuracy\",\"Total_Training_Time\",\"Latency\",\"Throughput\"]\n",
    "for i in x:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(compare_df[i])\n",
    "    plt.title(\"CPU vs GPU\")\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel(\"Value\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
